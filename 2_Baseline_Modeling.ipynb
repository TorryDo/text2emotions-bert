{"cells":[{"cell_type":"markdown","metadata":{"id":"KaNlFWOY2W3M"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"B2CWRIBh8ekZ"},"source":["\n","\n","*   In the previous step, we have cleanded our data\n","*   In this notebook, we will try to build a baseline model that detects one or multiple emotions in a text based on the GoEmotions data (multi-label text classification)\n","*   The score of pur baseline model will be used as a reference when building more complex models\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bmJO_oLA2ejV"},"source":["# 1 - Importing libraries and loading data"]},{"cell_type":"markdown","metadata":{"id":"Bn46rnObNtV_"},"source":["First, let's install and import some libraries for data exploration and  processing."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7628,"status":"ok","timestamp":1617217205862,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"OGeR81nwhnZs","outputId":"7b18e067-2736-4488-c421-5ba165e7a652"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["# Installing additional libraries for text preprocessing\n","%pip -q install emoji\n","%pip -q install contractions\n","%pip -q install scikit-learn"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jjd5oso_tAwP"},"outputs":[],"source":["# Data manipulation libraries\n","import pandas as pd\n","import numpy as np\n","import json\n","from pprint import pprint\n","\n","# Text processing libraries\n","import emoji\n","import re\n","import contractions\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","# Scikit-Learn packages\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"markdown","metadata":{"id":"pFJTxr7i-jgO"},"source":["Now, let's import our data."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8955,"status":"ok","timestamp":1617217207222,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"h2mHjyEm-fyv","outputId":"79dec385-d859-4645-d8f1-cdff90decbce"},"outputs":[{"name":"stdout","output_type":"stream","text":["(43410, 29)\n","(5426, 29)\n","(5427, 29)\n"]}],"source":["# Importing train, validation and test datasets with preprocessed texts and labels\n","train_GE = pd.read_csv(\"data2/train_clean.csv\")\n","val_GE = pd.read_csv(\"data2/val_clean.csv\")\n","test_GE = pd.read_csv(\"data2/test_clean.csv\")\n","\n","# Shape validation\n","print(train_GE.shape)\n","print(val_GE.shape)\n","print(test_GE.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8946,"status":"ok","timestamp":1617217207223,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"t5L92Ke6_To-","outputId":"dab891ef-77b7-4f81-dfe3-b7e261e94040"},"outputs":[{"name":"stdout","output_type":"stream","text":["admiration\n","amusement\n","anger\n","annoyance\n","approval\n","caring\n","confusion\n","curiosity\n","desire\n","disappointment\n","disapproval\n","disgust\n","embarrassment\n","excitement\n","fear\n","gratitude\n","grief\n","joy\n","love\n","nervousness\n","optimism\n","pride\n","realization\n","relief\n","remorse\n","sadness\n","surprise\n","neutral\n"]}],"source":["# Loading emotion labels for GoEmotions taxonomy\n","with open(\"data/emotions.txt\", \"r\") as file:\n","    GE_taxonomy = file.read().split(\"\\n\")\n","\n","for emo in GE_taxonomy:\n","  print(emo)"]},{"cell_type":"markdown","metadata":{"id":"l2KyRDetNXm5"},"source":["# 2 - Preprocessings and transformations"]},{"cell_type":"markdown","metadata":{"id":"Yde1mZgyN1wj"},"source":["Before defining and constructing a baseline model, we need to perform some additional processings such as tokenizing and lemmatizing our samples."]},{"cell_type":"markdown","metadata":{"id":"F4YpoidgOZAH"},"source":["## 2.1 - Additional preprocessings for basic Machine Learning tasks"]},{"cell_type":"markdown","metadata":{"id":"lQaFs5AQOj_e"},"source":["First, let's remove all punctuations."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eVUDXY5tO9ZJ"},"outputs":[],"source":["# Additional text preprocessing\n","train_GE['Clean_text'] = train_GE['Clean_text'].apply(lambda x: re.sub(r\"[^A-Za-z_]+\",\" \", x))\n","test_GE['Clean_text'] = test_GE['Clean_text'].apply(lambda x: re.sub(r\"[^A-Za-z_]+\",\" \", x))"]},{"cell_type":"markdown","metadata":{"id":"oWCcHThxPdUA"},"source":["New we can tokenize our samples using spacy and more specifically the english model. After creating these tokens, we will be able to lemmatize them and remove english stop words that may not help us in the classification task."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14010,"status":"ok","timestamp":1617217212303,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"vcznKVRpOu2v","outputId":"cf78ba50-63d4-49db-b927-c2d65f5cd5ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["# Download model \n","!python -m spacy download en_core_web_sm -q"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"bc_hi-ipP_dE"},"outputs":[],"source":["# Import English using en_core_web_sm.load()\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"wiAEGmEqQD98"},"outputs":[],"source":["# Creating tokenized documents\n","tokenized_train_GE = train_GE[\"Clean_text\"].apply(lambda desc: nlp(desc))\n","tokenized_test_GE = test_GE[\"Clean_text\"].apply(lambda desc: nlp(desc))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"KXy9aEC3QVmd"},"outputs":[],"source":["# Lemmatize each token and removing english stopwords\n","tokenized_train_GE = tokenized_train_GE.apply(lambda x: [token.lemma_ for token in x if token.lemma_ not in STOP_WORDS])\n","tokenized_test_GE = tokenized_test_GE.apply(lambda x: [token.lemma_ for token in x if token.lemma_ not in STOP_WORDS])\n","\n","# Creating clean data in our dataframes\n","train_GE[\"Clean_token\"] = [\" \".join(x) for x in tokenized_train_GE]\n","test_GE[\"Clean_token\"] = [\" \".join(x) for x in tokenized_test_GE]"]},{"cell_type":"markdown","metadata":{"id":"reuWDr-ISwcd"},"source":["## 2.2 - Create TF-IDF matrix"]},{"cell_type":"markdown","metadata":{"id":"COHnSzRvS83t"},"source":["Finally, we can create a TF-IDF matrix that will help us represent each sample of our corpus using the importance and frequency of each word in the sample, but also in the whole corpus."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571867,"status":"ok","timestamp":1617217770193,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"ITZzFL0KUqJa","outputId":"dd9d9c67-1a4f-4ff1-acb1-03ee68111ea1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(43410, 1000)\n","(5427, 1000)\n"]}],"source":["# TF-IDF vector with 1000 words vocabulary \n","vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n","\n","# Fitting the vectorizer and transforming train and test data\n","tfidf_train_GE = vectorizer.fit_transform(train_GE['Clean_token'])\n","tfidf_test_GE = vectorizer.transform(test_GE['Clean_token'])\n","\n","# Transforming from generators to arrays\n","tfidf_train_GE = tfidf_train_GE.toarray()\n","tfidf_test_GE = tfidf_test_GE.toarray()\n","\n","# Validating the shape of train and test data\n","print(tfidf_train_GE.shape)\n","print(tfidf_test_GE.shape)"]},{"cell_type":"markdown","metadata":{"id":"SfwfHUghUqI_"},"source":["The `max_features` argument in the `TfidfVectorizer` allows the maximum number of words to be considered in the vocabulary. Therefore, each sample in the train and test datasets will be represented using a vector of dimension `(1,1000)`.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dAr15JAoVY0A"},"source":["## 2.3 - Train and test variables"]},{"cell_type":"markdown","metadata":{"id":"eWLKT7opVgmY"},"source":["Let's define some explicit variables that will be used in constructing a machine learning model."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571860,"status":"ok","timestamp":1617217770194,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"bIINuxk7KZxJ","outputId":"bb052e3c-499d-48f2-a4c5-aca60860d8b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The shape of X_train is :  (43410, 1000)\n","The shape of y_train is :  (43410, 28)\n","\n","The shape of X_test is :  (5427, 1000)\n","The shape of y_test is :  (5427, 28)\n"]}],"source":["# Defining train and test variables\n","X_train =  tfidf_train_GE\n","y_train = train_GE.loc[:,GE_taxonomy].values\n","\n","X_test =  tfidf_test_GE\n","y_test = test_GE.loc[:,GE_taxonomy].values\n","\n","# Shape validation\n","print(\"The shape of X_train is : \", X_train.shape)\n","print(\"The shape of y_train is : \", y_train.shape)\n","print()\n","print(\"The shape of X_test is : \", X_test.shape)\n","print(\"The shape of y_test is : \", y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"6kYzU4f8V9Za"},"source":["# 3 - Dummy model"]},{"cell_type":"markdown","metadata":{"id":"HTC3soTRYkeT"},"source":["## 3.1 - Simulating dummy predictions"]},{"cell_type":"markdown","metadata":{"id":"AG6-PL7UWCi2"},"source":["Before creating a baseline model, we can try and simulate a **\"dummy model\"** that will **always detect the same emotions**, regardless of the sample. In our case, the dummy model could always predict the **'Neutral'** emotion as it is the most represented class in our train dataset."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"elapsed":571851,"status":"ok","timestamp":1617217770195,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"zlY_dBVjXZdk","outputId":"c64e6755-73e8-453a-80b5-e2807b47bf7b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Clean_text</th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity</th>\n","      <th>desire</th>\n","      <th>...</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","      <th>Clean_token</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>my favourite food is anything i did not have t...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>favourite food I cook</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>now if he does off himself everyone will think...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>think s laugh screw people instead actually dead</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>why the fuck is bayless isoing</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>fuck bayless isoing</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows Ã— 30 columns</p>\n","</div>"],"text/plain":["                                          Clean_text  admiration  amusement  \\\n","0  my favourite food is anything i did not have t...           0          0   \n","1  now if he does off himself everyone will think...           0          0   \n","2                     why the fuck is bayless isoing           0          0   \n","\n","   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n","0      0          0         0       0          0          0       0  ...   \n","1      0          0         0       0          0          0       0  ...   \n","2      1          0         0       0          0          0       0  ...   \n","\n","   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n","0            0         0      0            0       0        0        0   \n","1            0         0      0            0       0        0        0   \n","2            0         0      0            0       0        0        0   \n","\n","   surprise  neutral                                       Clean_token  \n","0         0        1                             favourite food I cook  \n","1         0        1  think s laugh screw people instead actually dead  \n","2         0        0                               fuck bayless isoing  \n","\n","[3 rows x 30 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# Preview of data\n","display(train_GE.head(3))"]},{"cell_type":"markdown","metadata":{"id":"GClM7Si8W2tr"},"source":["Without training an actual model, we can directly generate a predicitions matrice that mimics such a behaviour. The 'Neutral' emotion is the last emotion in our `GE_Taxonomy`list, therefore, it is also the last column in `y_train` and `y_test`."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571842,"status":"ok","timestamp":1617217770195,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"Ou3jw2pwI7XA","outputId":"e4e48b81-910e-4643-ebc3-278c1632c949"},"outputs":[{"data":{"text/plain":["array([[0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1]], dtype=int64)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Always predicting neutral emotion \n","dummy_preds = np.zeros_like(y_test)\n","dummy_preds[:,-1] = 1\n","dummy_preds"]},{"cell_type":"markdown","metadata":{"id":"fPDoeXNiYqTO"},"source":["## 3.2 - Evaluation on GoEmotions taxonomy"]},{"cell_type":"markdown","metadata":{"id":"3lC5GGd1ZTWz"},"source":["In order to evaluate the model, we will be using the f1-score. The f1-score allows to balance between recall and precision, which is very useful when it comes to unbalanced data."]},{"cell_type":"markdown","metadata":{"id":"LLFvYSmhZzZM"},"source":["We define a custom function that will compute the f1-score, precision and recall for each emotion, and also compute the macro-average of these metrics as a global metric."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"3CJMgmzj2XoN"},"outputs":[],"source":["# Model evaluation function \n","def model_eval(y_true, y_pred_labels, emotions):\n","    \n","    # Defining variables\n","    precision = []\n","    recall = []\n","    f1 = []\n","    \n","    # Per emotion evaluation      \n","    idx2emotion = {i: e for i, e in enumerate(emotions)}\n","    \n","    for i in range(len(emotions)):\n","   \n","        # Computing precision, recall and f1-score\n","        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n","        \n","        # Append results in lists\n","        precision.append(round(p, 2))\n","        recall.append(round(r, 2))\n","        f1.append(round(f1_score, 2))\n","    \n","    # Macro evaluation\n","    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n","    \n","    # Append results in lists\n","    precision.append(round(macro_p, 2))\n","    recall.append(round(macro_r, 2))\n","    f1.append(round(macro_f1_score, 2))\n","    \n","    # Converting results to a dataframe\n","    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n","    df_results.index = emotions+['MACRO-AVERAGE']\n","    \n","    return df_results"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":572214,"status":"ok","timestamp":1617217770583,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"rW_rNrE_cN56","outputId":"b6cbe288-d159-4681-9ca2-9e2be87fce39"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\trido\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>admiration</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>amusement</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>anger</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>annoyance</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>approval</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>caring</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>confusion</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>curiosity</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>desire</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>disappointment</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>disapproval</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>embarrassment</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>excitement</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>gratitude</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>grief</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>love</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>nervousness</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>optimism</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>pride</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>realization</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>relief</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>remorse</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>neutral</th>\n","      <td>0.33</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>MACRO-AVERAGE</th>\n","      <td>0.01</td>\n","      <td>0.04</td>\n","      <td>0.02</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Precision  Recall    F1\n","admiration           0.00    0.00  0.00\n","amusement            0.00    0.00  0.00\n","anger                0.00    0.00  0.00\n","annoyance            0.00    0.00  0.00\n","approval             0.00    0.00  0.00\n","caring               0.00    0.00  0.00\n","confusion            0.00    0.00  0.00\n","curiosity            0.00    0.00  0.00\n","desire               0.00    0.00  0.00\n","disappointment       0.00    0.00  0.00\n","disapproval          0.00    0.00  0.00\n","disgust              0.00    0.00  0.00\n","embarrassment        0.00    0.00  0.00\n","excitement           0.00    0.00  0.00\n","fear                 0.00    0.00  0.00\n","gratitude            0.00    0.00  0.00\n","grief                0.00    0.00  0.00\n","joy                  0.00    0.00  0.00\n","love                 0.00    0.00  0.00\n","nervousness          0.00    0.00  0.00\n","optimism             0.00    0.00  0.00\n","pride                0.00    0.00  0.00\n","realization          0.00    0.00  0.00\n","relief               0.00    0.00  0.00\n","remorse              0.00    0.00  0.00\n","sadness              0.00    0.00  0.00\n","surprise             0.00    0.00  0.00\n","neutral              0.33    1.00  0.50\n","MACRO-AVERAGE        0.01    0.04  0.02"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Model evaluation\n","model_eval(y_test, dummy_preds, GE_taxonomy)"]},{"cell_type":"markdown","metadata":{"id":"MygPwjp6dKxC"},"source":["As expected, **the model performs very poorly**. However, we can try to improve this score by implementing a baseline model using a simple machine learning classification model."]},{"cell_type":"markdown","metadata":{"id":"ehWZp50ReWv1"},"source":["# 4 - Baseline model: Ridge Classifier"]},{"cell_type":"markdown","metadata":{"id":"4YyEdLSde6-Y"},"source":["In this section, we will train a simple classification algorithm, the ridge classifier. However, this algorithm does not support multi-label classification. A simple strategy to do that consists of fitting one model per target using the `MultiOutputClassifier`."]},{"cell_type":"markdown","metadata":{"id":"ymA6YBshf49b"},"source":["## 4.1 - Training the model and evaluation on GoEmotions taxonomy"]},{"cell_type":"markdown","metadata":{"id":"1hfQ1tXjgJwt"},"source":["Let's create our model and fit it to our data. This a pretty simple model as it converts target variables to {-1,1} and trats the problem as a regular regression task."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":644509,"status":"ok","timestamp":1617217842890,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"uZRj1Pt5VI4C","outputId":"0c7ae13c-abec-4167-e34c-c16bd7452edc"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=RidgeClassifier(class_weight=&#x27;balanced&#x27;),\n","                      n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputClassifier</label><div class=\"sk-toggleable__content\"><pre>MultiOutputClassifier(estimator=RidgeClassifier(class_weight=&#x27;balanced&#x27;),\n","                      n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["MultiOutputClassifier(estimator=RidgeClassifier(class_weight='balanced'),\n","                      n_jobs=-1)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Multi-label classification \n","rc = RidgeClassifier(class_weight='balanced')\n","classifier = MultiOutputClassifier(rc, n_jobs=-1)\n","classifier.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646226,"status":"ok","timestamp":1617217844617,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"5KFEM7vJTTNn","outputId":"f390aae1-0439-4d9d-fa64-8bdd9099691d"},"outputs":[{"data":{"text/plain":["array([[0, 0, 0, ..., 1, 0, 0],\n","       [1, 0, 0, ..., 0, 0, 0],\n","       [1, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [1, 0, 0, ..., 0, 0, 0],\n","       [0, 1, 0, ..., 0, 0, 1]], dtype=int64)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Making predictions on GoEmotions taxonomy \n","classifier_preds = classifier.predict(X_test)\n","classifier_preds"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":948},"executionInfo":{"elapsed":485,"status":"ok","timestamp":1617217890782,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"jg-VzcqFsic-","outputId":"ed99cd03-c8af-4dc4-8ead-abb445fe0133"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>admiration</th>\n","      <td>0.39</td>\n","      <td>0.71</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>amusement</th>\n","      <td>0.58</td>\n","      <td>0.90</td>\n","      <td>0.71</td>\n","    </tr>\n","    <tr>\n","      <th>anger</th>\n","      <td>0.15</td>\n","      <td>0.66</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>annoyance</th>\n","      <td>0.13</td>\n","      <td>0.59</td>\n","      <td>0.22</td>\n","    </tr>\n","    <tr>\n","      <th>approval</th>\n","      <td>0.14</td>\n","      <td>0.66</td>\n","      <td>0.24</td>\n","    </tr>\n","    <tr>\n","      <th>caring</th>\n","      <td>0.08</td>\n","      <td>0.59</td>\n","      <td>0.14</td>\n","    </tr>\n","    <tr>\n","      <th>confusion</th>\n","      <td>0.07</td>\n","      <td>0.60</td>\n","      <td>0.12</td>\n","    </tr>\n","    <tr>\n","      <th>curiosity</th>\n","      <td>0.09</td>\n","      <td>0.58</td>\n","      <td>0.15</td>\n","    </tr>\n","    <tr>\n","      <th>desire</th>\n","      <td>0.10</td>\n","      <td>0.69</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>disappointment</th>\n","      <td>0.05</td>\n","      <td>0.48</td>\n","      <td>0.10</td>\n","    </tr>\n","    <tr>\n","      <th>disapproval</th>\n","      <td>0.09</td>\n","      <td>0.56</td>\n","      <td>0.15</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>0.10</td>\n","      <td>0.64</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>embarrassment</th>\n","      <td>0.03</td>\n","      <td>0.51</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>excitement</th>\n","      <td>0.08</td>\n","      <td>0.63</td>\n","      <td>0.14</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.15</td>\n","      <td>0.81</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>gratitude</th>\n","      <td>0.66</td>\n","      <td>0.91</td>\n","      <td>0.76</td>\n","    </tr>\n","    <tr>\n","      <th>grief</th>\n","      <td>0.03</td>\n","      <td>0.83</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.18</td>\n","      <td>0.78</td>\n","      <td>0.29</td>\n","    </tr>\n","    <tr>\n","      <th>love</th>\n","      <td>0.40</td>\n","      <td>0.82</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>nervousness</th>\n","      <td>0.02</td>\n","      <td>0.39</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>optimism</th>\n","      <td>0.16</td>\n","      <td>0.72</td>\n","      <td>0.26</td>\n","    </tr>\n","    <tr>\n","      <th>pride</th>\n","      <td>0.02</td>\n","      <td>0.56</td>\n","      <td>0.04</td>\n","    </tr>\n","    <tr>\n","      <th>realization</th>\n","      <td>0.06</td>\n","      <td>0.56</td>\n","      <td>0.11</td>\n","    </tr>\n","    <tr>\n","      <th>relief</th>\n","      <td>0.01</td>\n","      <td>0.27</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>remorse</th>\n","      <td>0.18</td>\n","      <td>0.82</td>\n","      <td>0.30</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.14</td>\n","      <td>0.70</td>\n","      <td>0.24</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>0.15</td>\n","      <td>0.69</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>neutral</th>\n","      <td>0.53</td>\n","      <td>0.81</td>\n","      <td>0.64</td>\n","    </tr>\n","    <tr>\n","      <th>MACRO-AVERAGE</th>\n","      <td>0.17</td>\n","      <td>0.66</td>\n","      <td>0.25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Precision  Recall    F1\n","admiration           0.39    0.71  0.50\n","amusement            0.58    0.90  0.71\n","anger                0.15    0.66  0.25\n","annoyance            0.13    0.59  0.22\n","approval             0.14    0.66  0.24\n","caring               0.08    0.59  0.14\n","confusion            0.07    0.60  0.12\n","curiosity            0.09    0.58  0.15\n","desire               0.10    0.69  0.18\n","disappointment       0.05    0.48  0.10\n","disapproval          0.09    0.56  0.15\n","disgust              0.10    0.64  0.18\n","embarrassment        0.03    0.51  0.05\n","excitement           0.08    0.63  0.14\n","fear                 0.15    0.81  0.25\n","gratitude            0.66    0.91  0.76\n","grief                0.03    0.83  0.05\n","joy                  0.18    0.78  0.29\n","love                 0.40    0.82  0.54\n","nervousness          0.02    0.39  0.03\n","optimism             0.16    0.72  0.26\n","pride                0.02    0.56  0.04\n","realization          0.06    0.56  0.11\n","relief               0.01    0.27  0.01\n","remorse              0.18    0.82  0.30\n","sadness              0.14    0.70  0.24\n","surprise             0.15    0.69  0.25\n","neutral              0.53    0.81  0.64\n","MACRO-AVERAGE        0.17    0.66  0.25"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Model evaluation\n","model_eval(y_test, classifier_preds, GE_taxonomy)"]},{"cell_type":"markdown","metadata":{"id":"qeCbA0_Lsu_K"},"source":["As we can see, our baseline model performs better than the dummy model. However, the score is still relatively low and can be improved using a more advanced model. This score will be used as a reference in the next steps."]},{"cell_type":"markdown","metadata":{"id":"zjMHKBd6pEl6"},"source":["## 4.2 - Make predictions"]},{"cell_type":"markdown","metadata":{"id":"sJABBqRVtlbk"},"source":["To make predictions on a new sample, it needs to be processed using all the different precessing steps we used."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"tdTRJR_ZhVKI"},"outputs":[],"source":["# Retrieving initial text preprocessings\n","def preprocess_corpus(x):\n","    \n","    # Adding a space between words and punctation\n","    x = re.sub( r'([a-zA-Z\\[\\]])([,;.!?])', r'\\1 \\2', x)\n","    x = re.sub( r'([,;.!?])([a-zA-Z\\[\\]])', r'\\1 \\2', x)\n","\n","    # Demojize\n","    x = emoji.demojize(x)\n","\n","    # Expand contraction\n","    x = contractions.fix(x)\n","\n","    # Lower\n","    x = x.lower()\n","\n","    #correct some acronyms/typos/abbreviations  \n","    x = re.sub(r\"lmao\", \"laughing my ass off\", x)  \n","    x = re.sub(r\"amirite\", \"am i right\", x)\n","    x = re.sub(r\"\\b(tho)\\b\", \"though\", x)\n","    x = re.sub(r\"\\b(ikr)\\b\", \"i know right\", x)\n","    x = re.sub(r\"\\b(ya|u)\\b\", \"you\", x)\n","    x = re.sub(r\"\\b(eu)\\b\", \"europe\", x)\n","    x = re.sub(r\"\\b(da)\\b\", \"the\", x)\n","    x = re.sub(r\"\\b(dat)\\b\", \"that\", x)\n","    x = re.sub(r\"\\b(dats)\\b\", \"that is\", x)\n","    x = re.sub(r\"\\b(cuz)\\b\", \"because\", x)\n","    x = re.sub(r\"\\b(fkn)\\b\", \"fucking\", x)\n","    x = re.sub(r\"\\b(tbh)\\b\", \"to be honest\", x)\n","    x = re.sub(r\"\\b(tbf)\\b\", \"to be fair\", x)\n","    x = re.sub(r\"faux pas\", \"mistake\", x)\n","    x = re.sub(r\"\\b(btw)\\b\", \"by the way\", x)\n","    x = re.sub(r\"\\b(bs)\\b\", \"bullshit\", x)\n","    x = re.sub(r\"\\b(kinda)\\b\", \"kind of\", x)\n","    x = re.sub(r\"\\b(bruh)\\b\", \"bro\", x)\n","    x = re.sub(r\"\\b(w/e)\\b\", \"whatever\", x)\n","    x = re.sub(r\"\\b(w/)\\b\", \"with\", x)\n","    x = re.sub(r\"\\b(w/o)\\b\", \"without\", x)\n","    x = re.sub(r\"\\b(doj)\\b\", \"department of justice\", x)\n","\n","    # replace some words with multiple occurences of a letter, example \"coooool\" turns into --> cool\n","    x = re.sub(r\"\\b(j+e{2,}z+e*)\\b\", \"jeez\", x)\n","    x = re.sub(r\"\\b(co+l+)\\b\", \"cool\", x)\n","    x = re.sub(r\"\\b(g+o+a+l+)\\b\", \"goal\", x)\n","    x = re.sub(r\"\\b(s+h+i+t+)\\b\", \"shit\", x)\n","    x = re.sub(r\"\\b(o+m+g+)\\b\", \"omg\", x)\n","    x = re.sub(r\"\\b(w+t+f+)\\b\", \"wtf\", x)\n","    x = re.sub(r\"\\b(w+h+a+t+)\\b\", \"what\", x)\n","    x = re.sub(r\"\\b(y+e+y+|y+a+y+|y+e+a+h+)\\b\", \"yeah\", x)\n","    x = re.sub(r\"\\b(w+o+w+)\\b\", \"wow\", x)\n","    x = re.sub(r\"\\b(w+h+y+)\\b\", \"why\", x)\n","    x = re.sub(r\"\\b(s+o+)\\b\", \"so\", x)\n","    x = re.sub(r\"\\b(f)\\b\", \"fuck\", x)\n","    x = re.sub(r\"\\b(w+h+o+p+s+)\\b\", \"whoops\", x)\n","    x = re.sub(r\"\\b(ofc)\\b\", \"of course\", x)\n","    x = re.sub(r\"\\b(the us)\\b\", \"usa\", x)\n","    x = re.sub(r\"\\b(gf)\\b\", \"girlfriend\", x)\n","    x = re.sub(r\"\\b(hr)\\b\", \"human ressources\", x)\n","    x = re.sub(r\"\\b(mh)\\b\", \"mental health\", x)\n","    x = re.sub(r\"\\b(idk)\\b\", \"i do not know\", x)\n","    x = re.sub(r\"\\b(gotcha)\\b\", \"i got you\", x)\n","    x = re.sub(r\"\\b(y+e+p+)\\b\", \"yes\", x)\n","    x = re.sub(r\"\\b(a*ha+h[ha]*|a*ha +h[ha]*)\\b\", \"haha\", x)\n","    x = re.sub(r\"\\b(o?l+o+l+[ol]*)\\b\", \"lol\", x)\n","    x = re.sub(r\"\\b(o*ho+h[ho]*|o*ho +h[ho]*)\\b\", \"ohoh\", x)\n","    x = re.sub(r\"\\b(o+h+)\\b\", \"oh\", x)\n","    x = re.sub(r\"\\b(a+h+)\\b\", \"ah\", x)\n","    x = re.sub(r\"\\b(u+h+)\\b\", \"uh\", x)\n","\n","    # Handling emojis\n","    x = re.sub(r\"<3\", \" love \", x)\n","    x = re.sub(r\"xd\", \" smiling_face_with_open_mouth_and_tightly_closed_eyes \", x)\n","    x = re.sub(r\":\\)\", \" smiling_face \", x)\n","    x = re.sub(r\"^_^\", \" smiling_face \", x)\n","    x = re.sub(r\"\\*_\\*\", \" star_struck \", x)\n","    x = re.sub(r\":\\(\", \" frowning_face \", x)\n","    x = re.sub(r\":\\^\\(\", \" frowning_face \", x)\n","    x = re.sub(r\";\\(\", \" frowning_face \", x)\n","    x = re.sub(r\":\\/\",  \" confused_face\", x)\n","    x = re.sub(r\";\\)\",  \" wink\", x)\n","    x = re.sub(r\">__<\",  \" unamused \", x)\n","    x = re.sub(r\"\\b([xo]+x*)\\b\", \" xoxo \", x)\n","    x = re.sub(r\"\\b(n+a+h+)\\b\", \"no\", x)\n","    \n","    # Handling special cases of text\n","    x = re.sub(r\"h a m b e r d e r s\", \"hamberders\", x)\n","    x = re.sub(r\"b e n\", \"ben\", x)\n","    x = re.sub(r\"s a t i r e\", \"satire\", x)\n","    x = re.sub(r\"y i k e s\", \"yikes\", x)\n","    x = re.sub(r\"s p o i l e r\", \"spoiler\", x)\n","    x = re.sub(r\"thankyou\", \"thank you\", x)\n","    x = re.sub(r\"a^r^o^o^o^o^o^o^o^n^d\", \"around\", x)\n","\n","    # Remove special characters and numbers replace by space + remove double space\n","    x = re.sub(r\"\\b([.]{3,})\",\" dots \", x)\n","    x = re.sub(r\"[^A-Za-z!?_]+\",\" \", x)\n","    x = re.sub(r\"\\b([s])\\b *\",\"\", x)\n","    x = re.sub(r\" +\",\" \", x)\n","    x = x.strip()\n","\n","    return x     "]},{"cell_type":"markdown","metadata":{"id":"U90nYuBLuQkQ"},"source":["Now, let's define a function that makes predictions based on a text sample."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Bm-6OEi_1V6t"},"outputs":[],"source":["def predict_samples(text_samples, model):\n","\n","    # Text preprocessing and cleaning\n","    text_samples = pd.Series(text_samples)\n","    text_samples_clean = text_samples.apply(preprocess_corpus)\n","    \n","    # Create tfidf representation\n","    tfidf_text_samples_clean = vectorizer.transform(text_samples_clean)\n","    \n","    # labels predictions\n","    samples_pred_labels = model.predict(tfidf_text_samples_clean)\n","    samples_pred_labels_df = pd.DataFrame(samples_pred_labels)\n","    samples_pred_labels_df = samples_pred_labels_df.apply(lambda x: [GE_taxonomy[i] for i in range(len(x)) if x[i]==1], axis=1)\n","    \n","    return pd.DataFrame({\"Text\":text_samples, \"Emotions\":list(samples_pred_labels_df)})"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"elapsed":1580,"status":"ok","timestamp":1617218473739,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"},"user_tz":-120},"id":"rQ_dctb0O-9A","outputId":"7711d89d-1509-46ac-8a9c-dee9ba98b173"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Emotions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I'm on the vacation, the weather is nice, but ...</td>\n","      <td>[admiration, confusion, embarrassment]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Text  \\\n","0  I'm on the vacation, the weather is nice, but ...   \n","\n","                                 Emotions  \n","0  [admiration, confusion, embarrassment]  "]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Predict samples\n","predict_samples(\"no one cares my guy\", classifier)"]},{"cell_type":"markdown","metadata":{"id":"aVYWcPhtu8_3"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"dwogMwfhvAnF"},"source":["*   In this notebook, we constructed a dummy model that always predicts the 'Neutral' emotion. Given that this emotion is the most represented, the \"model\" has reasonable performances when it comes to detecting 'Neutral', but has poor global performances.\n","* The baseline model we trained allowed an increase in the score but it is still very low. This can be due to the fact that it considers the words in a text sample only according to their importance, and does not put them in their context (a sample is a combination of independent words)\n","*  In the next step, we are going to be using an algorithme that adresses the latter issue usinf the mechanism of 'attention': The BERT model"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPb5aj+w0o2m+ZYyWpTkR7n","collapsed_sections":[],"name":"2_Baseline_Modeling.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
